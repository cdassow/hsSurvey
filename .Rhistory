N <- 10^6
R <- 0; I <- 1; S <- N - I - R
g <- 1/(13/365)
b <- 1/50
age <- 10
R0 <- 1 + 1/(b*age)
B <- R0 * (g + b) / N
parms <- c(B = B, g = g, b = b, m=b)
years <- seq(0,30, by=.1)
SIRbd.out <- data.frame(ode(c(S=S,I=I,R=R), years, SIRbd, parms, hmax=.01))
matplot(SIRbd.out[,1], sqrt(SIRbd.out[,-1]), type='l', col=1,
lty=1:3, ylab="sqrt(No. of Individuals)", xlab='Years')
legend('right', c('S','I','R'), lty=1:3, bty='n'
SIRbd <- function(t, y, p) {
S <- y[1]; I <- y[2]; R <- y[3]
with( as.list(p), {
dS.dt <- b*(S+I+R) - B*I*S - m*S
dI.dt <- B*I*S - g*I - m*I
dR.dt <- g*I - m*R
return( list(c(dS.dt, dI.dt, dR.dt)) )
} )
}
N <- 10^6
R <- 0; I <- 1; S <- N - I - R
g <- 1/(13/365)
b <- 1/50
age <- 50
R0 <- 1 + 1/(b*age)
B <- R0 * (g + b) / N
parms <- c(B = B, g = g, b = b, m=b)
years <- seq(0,30, by=.1)
SIRbd.out <- data.frame(ode(c(S=S,I=I,R=R), years, SIRbd, parms, hmax=.01))
matplot(SIRbd.out[,1], sqrt(SIRbd.out[,-1]), type='l', col=1,
lty=1:3, ylab="sqrt(No. of Individuals)", xlab='Years')
legend('right', c('S','I','R'), lty=1:3, bty='n'
#b)
SIRbd <- function(t, y, p) {
S <- y[1]; I <- y[2]; R <- y[3]
with( as.list(p), {
dS.dt <- b*(S+I+R) - B*I*S - m*S
dI.dt <- B*I*S - g*I - m*I
dR.dt <- g*I - m*R
return( list(c(dS.dt, dI.dt, dR.dt)) )
} )
}
N <- 10^6
R <- 0; I <- 1; S <- N - I - R
g <- 1/(13/365)
b <- 1/50
age <- 50
R0 <- 1 + 1/(b*age)
B <- R0 * (g + b) / N
parms <- c(B = B, g = g, b = b, m=b)
years <- seq(0,30, by=.1)
SIRbd.out <- data.frame(ode(c(S=S,I=I,R=R), years, SIRbd, parms, hmax=.01))
matplot(SIRbd.out[,1], sqrt(SIRbd.out[,-1]), type='l', col=1,
lty=1:3, ylab="sqrt(No. of Individuals)", xlab='Years')
legend('right', c('S','I','R'), lty=1:3, bty='n'
library(deSolve)
SIR.derivs<-function(curr.time, myvars, params) {
gamma = 1.36 #rate at whcih uninfected CD4 lymphocytes arise
tau = 0.2 # proportiono f cells activated
p = 0.1 # proportion of cells bcoming latently infected upon infection
beta = 0.00027 # rate of infection of CD4 lymphocyptes per viron
alpha = 3.6 * 10 ^ -2 # activation rate of latently infected cells
sigma = 2 # removal rat of cell-free virus
delta = 0.33 # removal (death) rate of actively infected CD4
pie = 100 # rate of production of virions by an actively infected cell
u = 1.36 * 10^-3 # HIV-indep death reate of unifected CD4 lymphocytes
R = myvars[1]
L = myvars[2]
E = myvars[3]
V = myvars[4]
dR = gamma * tau - u * R - beta * R * V
dL = p * beta * R * V - u * L - alpha * L
dE = (1-p) * beta * R * V + alpha * L - delta * E
dV = pie * E - sigma * V
return(list(c(dR, dL, dE, dV)))
}
# use libraries to solve SIR equations
library(deSolve)
vars.ini = c(1000, 0, 0, 100) ## initial value of R is 1000 and the initial of V is 100 - based on second paragraph
mytimes = c(1:100) * 0.1 #tau values for which you want to know the output
out = ode(vars.ini, mytimes, SIR.derivs, c(1))
out
plot(out, xlab = "time", ylab = c('R','L','E','V'), main = "Changes in Infection")
y=c(0, 1, 3. 9, 3, 1, 0)
y=c(0, 1, 3. 9, 3, 1, 0)
x = c(0, 1, 2, 3, 4, 5, 6)
y=c(0, 1, 3, 9, 3, 1, 0)
x = c(0, 1, 2, 3, 4, 5, 6)
data <- cbind(x, y)
data<-data.frame(data)
plot(data$x, data$y)
x=0
y=x^2 + x + 3
y
x=1
y=x^2 + x + 3
y
x=2
y=x^2 + x + 3
y
x=3
y=x^2 + x + 3
y
y=c(3, 5, 9, 9, 3, 1, 0)
x=4
y=x^2 + x + 3
y
x=5
y=x^2 + x + 3
y
x=6
y=x^2 + x + 3
y
x=-1
y=x^2 + x + 3
y
x=-2
y=x^2 + x + 3
y
y=c(5, 3, 3, 5, 9, 15, 23, 33, 45)
x = c(-2, -1, 0, 1, 2, 3, 4, 5, 6)
data <- cbind(x, y)
data<-data.frame(data)
plot(data$x, data$y)
z=y+rnorm(9,sd=2)
data <- cbind(x, y, z)
lm(z~x)
fit<-lm(z~x)
summary(fit)
residuals(fit)
plot(x, z, xlab = "x", ylab = "random values")
res<-residuals(fit)
plot(x, res, xlab = "x", ylab ="residuals")
summary(fit)
plot(fit)
plot(fit)
plot(x, z, xlab = "x", ylab = "random values")
abline(z~x)
plot(fit)
plot(data$x, data$y)
plot(data, x y)
plot(data, x, y)
View(data)
plot(data, x=data[,1], y=data[,2])
plot(x=data[,1], y=data[,2])
abline(fit)
plot(x=data[,1], y=data[,2])
abline(fit)
plot(x=data[,1], y=data[,2], xlab="x", ylab="y", main="observed data with lm fit")
abline(fit)
setwd("C:/Users/Camille/Desktop/Fishscapes/hsSurvey")
# load function to load data from google drive
source("gdriveURL.R")
library(dplyr)
######## angling CPUE
# load creel data from google drive
creel1=gdriveURL("https://drive.google.com/open?id=1lxUd742QZMXDQunyFBnENKMYZ1XNM_Pc")
creel2=gdriveURL("https://drive.google.com/open?id=1UYhbGH28WXjmi-4BzhfwO4KYwrBCNO2Q")
creel=rbind(creel1,creel2)
# reduce to columns we care about
creel=creel[,c(1,3,6,12,18,25:26,30,36,38)]
# calculate effort
# add zeroes to times with only 2 or 3 digits
creel$timeStart[nchar(creel$timeStart)==3]=paste("0",creel$timeStart[nchar(creel$timeStart)==3],sep="")
creel$timeStart[nchar(creel$timeStart)==2]=paste("00",creel$timeStart[nchar(creel$timeStart)==2],sep="")
creel$timeStart[creel$timeStart=="0"]="0000"
creel=creel[creel$timeStart!="1",]  # 4 entries with "1", so we don't know start time
creel$timeEnd[nchar(creel$timeEnd)==3]=paste("0",creel$timeEnd[nchar(creel$timeEnd)==3],sep="")
creel$timeEnd[nchar(creel$timeStart)==2]=paste("00",creel$timeEnd[nchar(creel$timeEnd)==2],sep="")
creel$timeEnd[creel$timeEnd=="0"]="0000"
creel$boatHrs=0
# remove rows when end time is less than start time (assumes the boat was out over midnight)
creel=creel[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M"),]
# calculate difference of time in hours for rows where end time is greater than start time (fishing occurred in one day only)
creel$boatHrs[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M")]=as.numeric(difftime(strptime(creel$timeEnd[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M")],format="%H%M"),strptime(creel$timeStart[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M")],format="%H%M"),units="hours"))
# removing rows with a non-zero notFishingAmt because we don't know what it means to be non-zero...
creel=creel[creel$notFishingAmt==0,]
# remove rows with non-integer anglersAmt
creel=creel[!grepl(".",creel$anglersAmt,fixed=TRUE),]
# remove rows with anglersAmt above 10? (arbitrary choice for now)
creel=creel[creel$anglersAmt<=10,]
# get angler hours of effort from party size and boat hours
creel$anglerHrs=creel$boatHrs*creel$anglersAmt
# remove rows with no species code
creel=creel[!is.na(creel$fishSpeciesCode),]
# remove rows with NA for caughtAmt
creel=creel[!is.na(creel$caughtAmt),]
# remove no effort (anglerHrs==0) rows
creel=creel[creel$anglerHrs>0,]
# calculate angling CPUE
creel$anglingCPUE=creel$caughtAmt/creel$anglerHrs
# removing instances of CPUE >=30 (arbitrary...)
creel=creel[creel$anglingCPUE<30,]
# calculate average angling CPUE and sample size for each lake-year-species combination
lake_yearCPUE=creel %>%
group_by(WBIC,fishSpeciesCode,surveyYear,county) %>%
summarize(meanCPUE=mean(anglingCPUE),
N=n())
lake_yearCPUE=as.data.frame(lake_yearCPUE)
####### electrofishing abundance
bassEF=gdriveURL("https://drive.google.com/open?id=11v8FbT2wnKx_CqUfxu_V9r_8fyCfcdD2")
bassEF=bassEF[,c(1,3,5,13,19,27:29)]
bassEF$CPEkm=bassEF$CPEmile/1.60934   # convert fish per mile to fish per km
bassEF$distanceShockedKm=bassEF$distanceShockedMiles*0.621371 # convert miles to km
lake_yearBASSef= bassEF %>%
group_by(WBIC,species,surveyYear,county) %>%
summarize(meanEF_CPEkm=mean(CPEkm),
totalFishCaught=sum(totalNumberCaughtFish),
totalDistShockedKm=sum(distanceShockedKm),
totalHoursSampled=sum(numberHoursSampled),
std=sd(CPEkm),
N=n())
lake_yearBASSef=as.data.frame(lake_yearBASSef)
panEF=gdriveURL("https://drive.google.com/open?id=1QIqCBQ9gbOgRFUJQbnokwwTZJi5VZZIR")
panEF=panEF[,c(1,3,5,13,19,25:27)]
panEF$CPEkm=panEF$CPEmile/1.60934   # convert fish per mile to fish per km
panEF$distanceShockedKm=panEF$distanceShockedMiles*0.621371 # convert miles to km
lake_yearPANef= panEF %>%
group_by(WBIC,species,surveyYear,county) %>%
summarize(meanEF_CPEkm=mean(CPEkm),
totalFishCaught=sum(totalNumberCaughtFish),
totalDistShockedKm=sum(distanceShockedKm),
totalHoursSampled=sum(numberHoursSampled),
std=sd(CPEkm),
N=n())
lake_yearPANef=as.data.frame(lake_yearPANef)
walleyeEF=gdriveURL("https://drive.google.com/open?id=1DPRROWv6Cf_fP6Z-kE9ZgUfdf_F_jSNT")
walleyeEF=walleyeEF[,c(1,3,5,13,19,23:24,27)]
walleyeEF$CPEkm=walleyeEF$CPEmile/1.60934   # convert fish per mile to fish per km
walleyeEF$distanceShockedKm=walleyeEF$distanceShockedMiles*0.621371 # convert miles to km
#remove commas from total fish caught
walleyeEF$totalNumberCaughtFish=as.numeric(gsub(",","",walleyeEF$totalNumberCaughtFish))
lake_yearWALLef= walleyeEF %>%
group_by(WBIC,species,surveyYear,county) %>%
summarize(meanEF_CPEkm=mean(CPEkm),
totalFishCaught=sum(totalNumberCaughtFish),
totalDistShockedKm=sum(distanceShockedKm),
totalHoursSampled=sum(numberHoursSampled),
std=sd(CPEkm),
N=n())
lake_yearWALLef=as.data.frame(lake_yearWALLef)
##### merge data sets from angling CPUE and electrofishing CPUE to get exact lake-year matches
# convert fishSpeciesCode in lake_yearCPUE to species (name from ef stuff)
lake_yearCPUE$species=""
lake_yearCPUE$species[lake_yearCPUE$fishSpeciesCode=="X22"]="WALLEYE"
lake_yearCPUE$species[lake_yearCPUE$fishSpeciesCode=="W11"]="SMALLMOUTH BASS"
lake_yearCPUE$species[lake_yearCPUE$fishSpeciesCode=="W12"]="LARGEMOUTH BASS"
lake_yearCPUE$species[lake_yearCPUE$fishSpeciesCode=="X15"]="YELLOW PERCH"
lake_yearCPUE$species[lake_yearCPUE$fishSpeciesCode=="W14"]="BLACK CRAPPIE"
lake_yearCPUE$species[lake_yearCPUE$fishSpeciesCode=="W09"]="BLUEGILL"
# trim species without EF data (can we get other species EF data?)
lake_yearCPUE=lake_yearCPUE[lake_yearCPUE$species!="",]
bassJoin=left_join(lake_yearBASSef,lake_yearCPUE,by=c("WBIC"="WBIC","species"="species","surveyYear"="surveyYear", "county"="county"))
bassJoin=bassJoin[!is.na(bassJoin$meanCPUE),]
panJoin=left_join(lake_yearPANef,lake_yearCPUE,by=c("WBIC"="WBIC","species"="species","surveyYear"="surveyYear", "county"="county"))
panJoin=panJoin[!is.na(panJoin$meanCPUE),]
wallJoin=left_join(lake_yearWALLef,lake_yearCPUE,by=c("WBIC"="WBIC","species"="species","surveyYear"="surveyYear", "county"="county"))
wallJoin=wallJoin[!is.na(wallJoin$meanCPUE),]
table(lake_yearCPUE$species)
nrow(lake_yearBASSef)
nrow(bassJoin)
nrow(lake_yearPANef)
nrow(panJoin)
nrow(lake_yearWALLef)
nrow(wallJoin)
bassJoin$logCPUE=log(bassJoin$meanCPUE)
bassJoin$logAbun=log(bassJoin$meanEF_CPEkm)
bassJoin<- bassJoin[is.na(bassJoin$logCPUE)==F,]
bassJoin<- bassJoin[bassJoin$logCPUE!=-Inf,]
wallJoin$logCPUE=log(wallJoin$meanCPUE)
wallJoin$logAbun=log(wallJoin$meanEF_CPEkm)
wallJoin<- wallJoin[wallJoin$logCPUE!=-Inf,]
panJoin$logCPUE=log(panJoin$meanCPUE)
panJoin$logAbun=log(panJoin$meanEF_CPEkm)
panJoin<- panJoin[panJoin$logCPUE!=-Inf,]
#join tables to compare species with lm model
LMBplusWall=rbind(bassJoin[bassJoin$species=="LARGEMOUTH BASS",],wallJoin)
#generate linear model to compare hyperstability of
LMBvsWall<-lm(LMBplusWall$logCPUE~LMBplusWall$logAbun*LMBplusWall$species)
summary(LMBvsWall)
#Bass vs panfish
LMBplusPan=rbind(bassJoin[bassJoin$species=="LARGEMOUTH BASS",],panJoin)
LMBvsPan<- lm(LMBplusPan$logCPUE~LMBplusPan$logAbun*LMBplusPan$species)
summary(LMBvsPan)
#Panfish vs walleye
PanplusWall=rbind(panJoin,wallJoin)
PanvsWall<-lm(PanplusWall$logCPUE~PanplusWall$logAbun*PanplusWall$species)
summary(PanvsWall)
#general linear model for bass, using glm function
fit1<-glm(bassJoin$logCPUE~bassJoin$logAbun)
summary(fit1)
LMBvsSMB<-lm(bassJoin$logCPUE~bassJoin$logAbun*bassJoin$species)
summary(LMBvsSMB)
#plotting bass fit and LMBvsSMB fits
plot(x=bassJoin$logAbun,y=bassJoin$logCPUE, main = "Hyperstability of Bass in WI (1995-2016)",
xlab = "Fish density (log ef CPUE)", ylab= "Catch rate (log angling CPUE)" )
abline(LMBvsSMB, col="red")
abline(fit1, col="blue")
legend("bottomright",paste("Fit = ",c("LMB vs SMB","Bass")), lty = 1, col = 1:2, bty = "n")
#ploting model with fit line bass log transformed abund. and CPUE
plot(x=bassJoin$logAbun,y=bassJoin$logCPUE)
abline(fit1)
#normal spcae plot of model fit to the data, exponential(intercept)*x^slope this is qN^B
#coefficients 2 is beta
plot(x=bassJoin$meanEF_CPEkm,y=bassJoin$meanCPUE)
plot(1:65,exp(fit1$coefficients[1])*(1:65)^fit1$coefficients[2], ylab="logCPUE (angling CPUE)", xlab="logAbun (efCPUE)", main = "Hyperstability of bass ")
ggplot(bassJoin,aes(bassJoin$meanEF_CPEkm,bassJoin$meanCPUE))+
geom_point(aes(colour = surveyYear))
ggplot(fit1,aes(bassJoin$meanEF_CPEkm,bassJoin$meanCPUE))+geom_smooth(model=lm)
#model for panfish, fit summary estimate 0.23189
fit2<-glm(panJoin$logCPUE~panJoin$logAbun)
summary(fit2)
#ploting model with fit line log trans. for panfish
plot(x=panJoin$logAbun,y=panJoin$logCPUE)
abline(fit2)
#subsetting to just look at the number of bluegill obs, may be useful later with BLG vs LMB
BLGJoin=panJoin[panJoin$species=="BLUEGILL",]
BLGfit<-glm(BLGJoin$logCPUE~BLGJoin$logAbun)
summary(BLGfit)
LMBplusBLG=rbind(bassJoin[bassJoin$species=="LARGEMOUTH BASS",],BLGJoin)
LMBvsBLG<-lm(LMBplusBLG$logCPUE~LMBplusBLG$logAbun*LMBplusBLG$species)
summary(LMBvsBLG)
#Bass vs bluegill
BassplusBLG=rbind(bassJoin,BLGJoin)
BassvsBLG<-lm(BassplusBLG$logCPUE~BassplusBLG$logAbun*BassplusBLG$species)
summary(BassvsBLG)#sign differences in effect just largemouth not smallmouth, not signif. diferent from Small
#Bluegill vs walleye
BLGplusWall=rbind(BLGJoin,wallJoin)
BLGvsWall<-lm(BLGplusWall$logCPUE~BLGplusWall$logAbun*BLGplusWall$species)
summary(BLGvsWall)
#normal spcae plot of model fit to the data, qN^B
#coefficients 2 is beta
plot(x=panJoin$meanEF_CPEkm,y=panJoin$meanCPUE)
plot(0:160,exp(fit2$coefficients[1])*(0:160)^fit2$coefficients[2])
# glmodel for walleye, fit summary estimate 0.63073
fit3<-glm(wallJoin$logCPUE~wallJoin$logAbun)
summary(fit3)
#ploting model with fit line bass log transformed abund. and CPUE
plot(x=wallJoin$logAbun,y=wallJoin$logCPUE)
abline(fit3)
#normal spcae plot of model fit to the data, exponential(intercept)*x^slope this is qN^B
#coefficients 2 is beta
plot(x=wallJoin$meanEF_CPEkm,y=wallJoin$meanCPUE)
plot(1:165,exp(fit3$coefficients[1])*(1:165)^fit3$coefficients[2])
### Ploting hyperstability ###
plot(x=1:165,y=exp(fit1$coefficients[1])*(1:165)^fit1$coefficients[2], col='blue', type = "l",ylim = c(0,5),
main = "Hyperstability of fish Species in WI")
lines(1:165,exp(fit2$coefficients[1])*(1:165)^fit2$coefficients[2],col="red")
lines(1:165,exp(fit3$coefficients[1])*(1:165)^fit3$coefficients[2],col="darkgreen")
legend("topright",paste("Fit = ",c("LMB","Panfish","Walleye")), lty = 1:5, col = 1:5)
#make d the dataframe you want, using bass as example
d=panJoin
z=d[!duplicated(d$meanEF_CPEkm),]
agg_logCPUE=log(z$meanCPUE)
#wallJoin and panJoin agg_logCPUE has Inf value, needs to be removed for glm fit
agg_logN=log(z$meanEF_CPEkm)
aggFit_wLK=glm(agg_logCPUE~agg_logN)
#estimate agg fit glm summary 0.42386
summary(aggFit_wLK)
betas=numeric(1000) #betas from model fit to simulated data
ps=numeric(1000) #difference in AIC values between the simulated data model fit and the experimental data model fit
for(i in 1:1000){
pe=rlnorm(n=length(z$meanEF_CPEkm), meanlog = log(z$meanEF_CPEkm))
#rlnorm from chpt 14-45 of RMark book
fit=glm(agg_logCPUE ~ log(pe)+z$meanCPUE)
betas[i]=fit$coefficients[2]
comp=abs(fit$aic - aggFit_wLK$aic)
ps[i]=comp
}
plot(betas, ps)
hist(betas, main = "walleye betas")
hist(betas, main = "pan betas")
hist(ps)
#add in Building density *only for year 2016
buildDensity2016=gdriveURL("https://drive.google.com/open?id=11lPPduqiXIxz00fm6xxFzUA8u9nCOBnN")
#bringing in ntl buidling info for 2001-2004
NTLBuild<- read.csv("NTLBuildDensData(2001-2004).csv")
#fixing column names to join tables
NTLBuild$WBIC=NTLBuild$wbic
NTLBuild$surveyYear=NTLBuild$survey_year
NTLBuild<-NTLBuild[,c(1:4,8:34)]
#look at change in building density over time
#calculating buildings per km for 2016 data
buildDensity2016$buildings_per_km2016=buildDensity2016$buildingCount50m/(buildDensity2016$lakePerimeter_m*0.001)
buildDensCompare=full_join(buildDensity2016,NTLBuild, by="WBIC")
buildDensCompare=buildDensCompare[!is.na(buildDensCompare$buildings_per_km.y),]
plot(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x, xlab= "NTL estiamte (2001-2004)",
ylab="2016 estimate (GIS)", main="Lake building density comparison of different years")
#add in Building density *only for year 2016
buildDensity2016=gdriveURL("https://drive.google.com/open?id=11lPPduqiXIxz00fm6xxFzUA8u9nCOBnN")
#bringing in ntl buidling info for 2001-2004
NTLBuild<- read.csv("NTLBuildDensData(2001-2004).csv")
#fixing column names to join tables
NTLBuild$WBIC=NTLBuild$wbic
NTLBuild$surveyYear=NTLBuild$survey_year
NTLBuild<-NTLBuild[,c(1:4,8:34)]
#look at change in building density over time
#calculating buildings per km for 2016 data
buildDensity2016$buildings_per_km2016=buildDensity2016$buildingCount50m/(buildDensity2016$lakePerimeter_m*0.001)
buildDensCompare=full_join(buildDensity2016,NTLBuild, by="WBIC")
View(buildDensity2016)
#add in Building density *only for year 2016
buildDensity2016=gdriveURL("https://drive.google.com/open?id=11lPPduqiXIxz00fm6xxFzUA8u9nCOBnN")
#bringing in ntl buidling info for 2001-2004
NTLBuild<- read.csv("NTLBuildDensData(2001-2004).csv")
#fixing column names to join tables
NTLBuild$WBIC=NTLBuild$wbic
NTLBuild$surveyYear=NTLBuild$survey_year
NTLBuild<-NTLBuild[,c(1:4,8:34)]
#look at change in building density over time
#calculating buildings per km for 2016 data
buildDensity2016$buildings_per_km2016=buildDensity2016$buildingCount50m/(buildDensity2016$lakePerimeter_m*0.001)
buildDensCompare=full_join(buildDensity2016,NTLBuild, by="WBIC")
buildDensCompare=buildDensCompare[!is.na(buildDensCompare$buildings_per_km.y),]
plot(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x, xlab= "NTL estiamte (2001-2004)",
ylab="2016 estimate (GIS)", main="Lake building density comparison of different years")
#look at change in building density over time
#calculating buildings per km for 2016 data
buildDensity2016$buildings_per_km2016=buildDensity2016$buildingCount50m/(buildDensity2016$lakePerimeter_m*0.001)
#add in Building density *only for year 2016
buildDensity2016=gdriveURL("https://drive.google.com/open?id=11lPPduqiXIxz00fm6xxFzUA8u9nCOBnN")
#add in Building density *only for year 2016
buildDensity2016=gdriveURL("https://drive.google.com/open?id=11lPPduqiXIxz00fm6xxFzUA8u9nCOBnN")
#bringing in ntl buidling info for 2001-2004
NTLBuild<- read.csv("NTLBuildDensData(2001-2004).csv")
#fixing column names to join tables
NTLBuild$WBIC=NTLBuild$wbic
NTLBuild$surveyYear=NTLBuild$survey_year
NTLBuild<-NTLBuild[,c(1:4,8:34)]
View(buildDensity2016)
#look at change in building density over time
#calculating buildings per km for 2016 data
buildDensity2016$buildings_per_km2016=buildDensity2016$buildingCount50m/(buildDensity2016$lakePerimeter_m*0.001)
buildDensCompare=full_join(buildDensity2016,NTLBuild, by="WBIC")
buildDensCompare=buildDensCompare[!is.na(buildDensCompare$buildings_per_km.y),]
buildDensCompare=buildDensCompare[!is.na(buildDensCompare$buildings_per_km.x),]
plot(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x, xlab= "NTL estiamte (2001-2004)",
ylab="2016 estimate (GIS)", main="Lake building density comparison of different years")
ggplot(data=buildDensCompare, aes(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x))+geom_smooth()
library(ggplot2)
ggplot(data=buildDensCompare, aes(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x))+geom_smooth()
ggplot(data=buildDensCompare, aes(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x))
+geom_smooth()
ggplot(data=buildDensCompare, aes(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x))+geom_smooth()
plot(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x, xlab= "NTL estiamte (2001-2004)",
ylab="2016 estimate (GIS)", main="Lake building density comparison of different years")
ggplot(data=buildDensCompare, aes(x=buildDensCompare$buildings_per_km.y,y=buildDensCompare$buildings_per_km.x))+geom_smooth()
#joining building density to bass catch + abund info
bassbuildJoin=left_join(bassJoin,NTLBuild, by="WBIC","surveyYear")
View(creel)
unique(lake_yearCPUE$fishSpeciesCode=="L03")
unique(lake_yearCPUE$fishSpeciesCode=="LO3")
unique(lake_yearCPUE$fishSpeciesCode)
######## angling CPUE
# load creel data from google drive
creel1=gdriveURL("https://drive.google.com/open?id=1lxUd742QZMXDQunyFBnENKMYZ1XNM_Pc")
creel2=gdriveURL("https://drive.google.com/open?id=1UYhbGH28WXjmi-4BzhfwO4KYwrBCNO2Q")
creel=rbind(creel1,creel2)
unique(creel$fishSpeciesCode)
# reduce to columns we care about
creel=creel[,c(1,3,6,12,18,25:26,30,36,38)]
# calculate effort
# add zeroes to times with only 2 or 3 digits
creel$timeStart[nchar(creel$timeStart)==3]=paste("0",creel$timeStart[nchar(creel$timeStart)==3],sep="")
creel$timeStart[nchar(creel$timeStart)==2]=paste("00",creel$timeStart[nchar(creel$timeStart)==2],sep="")
creel$timeStart[creel$timeStart=="0"]="0000"
creel=creel[creel$timeStart!="1",]  # 4 entries with "1", so we don't know start time
creel$timeEnd[nchar(creel$timeEnd)==3]=paste("0",creel$timeEnd[nchar(creel$timeEnd)==3],sep="")
creel$timeEnd[nchar(creel$timeStart)==2]=paste("00",creel$timeEnd[nchar(creel$timeEnd)==2],sep="")
creel$timeEnd[creel$timeEnd=="0"]="0000"
creel$boatHrs=0
# remove rows when end time is less than start time (assumes the boat was out over midnight)
creel=creel[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M"),]
# calculate difference of time in hours for rows where end time is greater than start time (fishing occurred in one day only)
creel$boatHrs[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M")]=as.numeric(difftime(strptime(creel$timeEnd[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M")],format="%H%M"),strptime(creel$timeStart[strptime(creel$timeEnd,format="%H%M")>=strptime(creel$timeStart,format="%H%M")],format="%H%M"),units="hours"))
# removing rows with a non-zero notFishingAmt because we don't know what it means to be non-zero...
creel=creel[creel$notFishingAmt==0,]
# remove rows with non-integer anglersAmt
creel=creel[!grepl(".",creel$anglersAmt,fixed=TRUE),]
# remove rows with anglersAmt above 10? (arbitrary choice for now)
creel=creel[creel$anglersAmt<=10,]
# get angler hours of effort from party size and boat hours
creel$anglerHrs=creel$boatHrs*creel$anglersAmt
# remove rows with no species code
creel=creel[!is.na(creel$fishSpeciesCode),]
# remove rows with NA for caughtAmt
creel=creel[!is.na(creel$caughtAmt),]
# remove no effort (anglerHrs==0) rows
creel=creel[creel$anglerHrs>0,]
# calculate angling CPUE
creel$anglingCPUE=creel$caughtAmt/creel$anglerHrs
# removing instances of CPUE >=30 (arbitrary...)
creel=creel[creel$anglingCPUE<30,]
# calculate average angling CPUE and sample size for each lake-year-species combination
lake_yearCPUE=creel %>%
group_by(WBIC,fishSpeciesCode,surveyYear,county) %>%
summarize(meanCPUE=mean(anglingCPUE),
N=n())
lake_yearCPUE=as.data.frame(lake_yearCPUE)
unique(lake_yearCPUE$fishSpeciesCode)
length(lake_yearCPUE$fishSpeciesCode="L0")
length(lake_yearCPUE$fishSpeciesCode="L03")
length(lake_yearCPUE$fishSpeciesCode=="L03")
Musk<-lake_yearCPUE$fishSpeciesCode=="L03"
Musk<-lake_yearCPUE[lake_yearCPUE$fishSpeciesCode=="L03",]
View(Musk)
unique(Musk$county)
unique(Musk$surveyYear)
range(unique(Musk$surveyYear))
